The companies using new methods to poll the EU referendum

2016-06-22

Follow John Burn-Murdoch

Polling has dominated coverage of the UK’s EU referendum for the last month, with the focus shifting between the polls’ reliability following their 2015 misfire, the methodological challenges pollsters face, and the Remain vs Leave race itself.

The polls in question have been carried out by members of the British Polling Council, familiar names to anybody who follows the numbers game ahead of UK elections. But these aren’t the only organisations trying to use data on public opinion to forecast the outcome on June 23.

Among the companies turning their hand to polling over recent weeks are an online survey firm, a global technology giant and even a cash machine operator.

Large scale online polling

Every day almost 3 million people take surveys on SurveyMonkey’s online platform. Spotting an opportunity to take advantage of the size and diversity of this audience, the company began offering its users the chance to participate in political surveys, and the results were good.

In 2012, SurveyMonkey’s polls correctly predicted the winner of all-but-two states in the US presidential election, and they have had similarly impressive records at a number of US elections since. Their first foray into British politics came last year, when theirs was the only one out of all polls in the final week before the vote to correctly predict a significant Conservative lead.

Today a SurveyMonkey poll put Remain ahead of Leave by 50 per cent to 47. Two days ago their figures were 49-48 in favour of Leave, and a week earlier their first EU poll had the two neck-and-neck on 48. Almost 4,000 people were surveyed for each poll.

The numbers suggest a swing towards Remain as polling day approaches, but SurveyMonkey declares a margin-of-error on its polls of 2 per cent — slightly lower than that of most mainstream pollsters — meaning all three of those polls could in fact represent exactly the same spread of opinions among the electorate.

Essentially, SurveyMonkey appears to be in-line with average results from other pollsters this past week: the race is too close to call.

What’s particularly interesting about SurveyMonkey’s approach is how it differs from those of the mainstream British pollsters.

The more established polling companies make two sets of adjustments to respondents’ answers. First, they weight the data for fundamental demographic information like age, sex, region, country-of-birth and education levels in order to ensure their final figures are representative of the UK’s adult population. Then there is an additional step where the demographically adjusted numbers are re-weighted for things like political party preference and stated likelihood of voting.

SurveyMonkey doesn’t make the second set of adjustments, since chief research officer Jon Cohen believes there is a “great risk in relying too much on models”.

Cohen believes SurveyMonkey’s approach of polling very large numbers of people is a more accurate way of gauging mass opinion than polling smaller samples and relying on purely statistical techniques to ensure that they are representative of the population as a whole.

SurveyMonkey is able to reach large numbers of people more easily than its mainstream equivalents because its existing user-base acts as a ready-made panel of prospective respondents, whereas others rely on actively recruiting and maintaining panels of interested individuals.

Cash for questions

An entirely different approach is that taken by Cardtronics, the operator of more than 16,000 ATMs around the UK, which has been asking people which way they would vote while they wait for the cash machine to deliver their money. The question disappears automatically when the transaction has completed, so answering the question is not a requirement.

Cardtronics has received more than 85,000 responses to the question from the 3.5m times it has been served on screen across three periods since March of this year. Assuming even as little as half of these have come from unique individuals, this sample size is an order of magnitude larger than those used by either SurveyMonkey or any of the mainstream pollsters.

In its most recent period of offering the referendum question on its ATMs — June 9 to 16 — the top-line numbers gave Leave a 52-48 advantage across the 86,585 responses submitted, almost exactly the same as the 51-49 Leave lead from the same method a week earlier.

Familiar regional patterns suggest the method works reasonably well. Cardtronics breaks down the results into five regions: the highest leads for Remain have tended to come in London and Scotland, followed by the Home Counties, whilst Western England and Yorkshire have shown large leads for Leave.

There are reasons to be skeptical of this approach, though: firstly, the characteristics of people who use ATMs and the locations of the machines themselves both limit the extent to which this method can be considered representative of the population at large. A 2014 Guardian investigation found that 300,000 of the poorest people in Britain lived more than 1km away from the nearest free-to-use ATM.

Second, responses are not filtered for quality — there is a chance some people may simply be hitting any answer to dismiss the question while they wait for their cash to arrive — but given the huge number of responses and the voluntary nature of taking part in the ‘poll’, it is unlikely that a significant portion of answers would be affected in this way.

Another reason to believe the question is being answered seriously is that non-political market research questions asked via the same ATMs have yielded much higher response rates, i.e people are evidently making an active decision on whether or not to answer a question related to the EU referendum. On the flip side, this also means the sample is likely to contain a higher share of politically engaged people than the electorate as a whole.

In its defence, Cardtronics makes no claim that the results of its survey should be compared to the surveys carried out by mainstream pollsters or the likes of SurveyMonkey, instead framing the whole exercise as “a simple ‘pulse check’ of the nation”, which is still in development.

Online search data

Google knows more about us than we would care to think, and that includes an estimation of what issues we most closely associate with the topic of the EU referendum.

Although it could compare searches for terms explicitly linked to the Leave and Remain camps, this would be unhelpful since the uncertainty around what might happen after a vote to leave the EU means that searches for information about the Leave campaign are always going to far outweigh searches for Remain.

What search data can tell us is the broader issues people are searching for when looking for information that might help them decide which way to vote. Google has indexed the broad topics that people are searching for in the context of the referendum, and broken them down by urban area. A score of 100 indicates the most-searched-for term among all searches in a given geography associated with the referendum, and zero the least.

Overall, the search data follows a similar pattern to other methods.On average, immigration is comfortably the most searched term among all searches that Google considers related to the topic of the EU referendum. In second place is either the NHS or economic topics such as trade or finance.

At a more local level, the associations between Google’s data and survey-based measures of opinions on different topics breaks down somewhat. For example, comparing the estimated extent to which people feel immigration has a negative impact on their area, against where immigration ranks among referendum-related Google searches, yields no statistical relationship of any kind.

The lack of any clear pattern here would suggest that the amount of ‘noise’ in search data reduces its predictive power at a local level. But with turnout expected to be high, and war being waged between the two campaigns over the veracity of their claims, perhaps the Google search trails of new or skeptical voters will prove after the fact to have been a more useful predictor than first thought.


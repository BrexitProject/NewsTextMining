What are we to make of the polls?

2016-06-22

Political journalists have become exceedingly wary of pollsters since they called last year’s general election wrongly. Are they right to be? The performance of major polling companies in this referendum could make or break their reputations – get it wrong again and the fury of Fleet Street will be unconstrained.

But there’s good reason to feel that hacks should have learned some lessons too – the first being, properly understand what it is that you are reporting. Here are a handful of key points to bear in mind when in the coming days we consider the pollsters’ performance in this referendum.

1) Herding?

Journalists are deeply suspicious of the tendency for different companies’ polls to cluster more closely around a particular result as an election campaign runs on. It happened last year – the subsequent British Polling Council inquiry found no evidence of deliberate herding – and seems to be happening this time too. In particular the campaign started off with a massive difference of up to 20 percentage points between surveys carried out using the telephone and those done online. That gap has pretty much closed in the last few weeks.

The fact that several leading pollsters are still tweaking their methodologies has also provoked suspicion among reporters.

Are phone pollsters steering their work towards that of online surveys?

 This is the kind of thing that turns bright and cheery young journalists into grumpy cynical old hacks.

2) Treatment of “don’t knows”.

As anyone who has been asked to choose between five brands of strawberry jam will know, it is fairly common not to know what one’s preference is when first surveyed. Different pollsters treat these floating voters in different ways, and that can significantly affect the headline Remain/Leave percentage support figures they report.

This week’s work by ORB – which found a stonking eight-point lead for Remain – included a useful explanation of how it treats “don’t knows”. It cited “the theory that undecided voters tend to vote in favour of the status quo” in justifying its decision to allocate three times more “don’t knows” into the Remain camp than to Leave.

But Britain hasn’t had a European referendum for four decades, so – unlike general elections – pollsters lack previous benchmarks to use for this. We simply do not know how the last-minute undecided voters will split. And for pollsters the referendum has come at the worst possible moment: while their methodology is still in flux after the general election, and without a stash of past voter behaviour data on which they can base their modelling.

3) Likelihood to vote.

Another methodological question pollsters must ask themselves is how they assess someone’s likelihood of actually turning out and casting a vote on the day. Leave voters are widely expected to be more committed, while bad weather or last-minute news events are more likely to deter Remain supporters. People lower down the socioeconomic scale are more likely to overstate their propensity to vote, pollsters say.

Since last year’s general election polling companies have placed more emphasis on voters’ past behaviour when assessing their voting intentions. Those who say they vote in every election will be included in fieldwork; those who say they only “usually” vote are excluded. This creates a survey group which is solidly likely to make it to the polls but the downside is that it may fail to capture wavering swing voters whose decision to go and vote – and which side they choose to back – could determine the outcome of the referendum.

4) Shy Remainers.

Leave supporters are much more eager to give their opinion than those leaning towards Remain, pollsters have found. ICM caused much mirth last month when it changed the way it carries out its online surveying after finding that its Friday evening samples were dominated by older men who were strongly pro-Leave. It started to stagger the times at which it sent out email invitations to participate, in a bid to reduce this “eurosceptic mansplaining” effect.

One of the main problems with last year’s general election polling was that it overcounted enthusiastic voters and did not find enough apathetic “maybes”. Polling companies are attempting to address this in their work on the referendum, but it remains to be seen whether they have succeeded.

Repeated call-backs are necessary to reach this group of people, who tend to lean heavily towards Remain. That is expensive and time-consuming. Here’s some work by BMG Research showing how Remain’s lead changed as they called individual survey responders back repeatedly.

5) Spurious accuracy.

Firstly, as scientists (and cats in boxes) know, any attempt to measure something inevitably involves some element of uncertainty. This range of accuracy is known as the error margin. In polling samples it looks like this:

That means that if a poll of 1,000 people found that Remain had 53% public support, that figure should be properly reported as 53+/-3 … in other words the actual result lies anywhere between 50% and 56%.

Here’s how the NatCen Social Research reported their findings this week, which used new methodology developed in the wake of last year’s election polling disaster:

See those error bars? Those are what journalists should pay more attention to.

As NIESR’s Jonathan Portes pointed out, that only covers the margin of error in perfectly random samples. Given that the pollsters are trying to find 1-2,000 people who are representative of the UK’s total 46.5m voters, it would be unsurprising if their samples were not perfectly random.

On top of that, there is also the confidence interval. Broadly, this is a way of estimating how likely it is that the findings of each poll actually do fall within that +/- error margin range. A 95% confidence interval means 19/20 polls will be within the stated error margin.

One in 20 of these polls is rogue. Can you tell which? No.

Some final thoughts …

I had a little Twitter rant about polling a couple of days ago.

Polling 1-2,000 people representative of the entire ~45m registered UK voters in 1-3 days w/small budget is v hard https://t.co/HEI91BXVzv

It’s not as though pollsters do not know what to do to get more accurate findings – they need time and money. That is how they carry out the largescale surveys for private and government clients which bring in the majority of their revenue. It is also how they carry out exit polls – last year’s being a striking example of how to get it right.

If either side wins with a margin of just a few percentage points, polling companies will actually have done pretty well – but they are still likely to be pilloried by reporters who do not understand just how hard small-sample, short-timescale political polling can be.

Would it be better for pollsters to stop doing this kind of work entirely and insist that media clients club together to fund fewer, and better, polls? That was one of the suggestions made by the post-election polling inquest. If they call it wrong tomorrow,it is something that may be forced upon them.

For many journalists, pollsters are drinking in the last-chance saloon.

